import OpenAI, { AzureOpenAI } from "openai";
import {
  ChatCompletionMessageParam,
  ChatCompletionTool,
} from "openai/resources";

import { kEnvs } from "../utils/env";
import { withDefault } from "../utils/base";
import { ChatCompletionCreateParamsBase } from "openai/resources/chat/completions";
import { Logger } from "../utils/log";
import { kProxyAgent } from "./proxy";
import { isNotEmpty } from "../utils/is";

export interface ChatOptions {
  user: string;
  system?: string;
  model?: ChatCompletionCreateParamsBase["model"];
  tools?: Array<ChatCompletionTool>;
  jsonMode?: boolean;
  requestId?: string;
  trace?: boolean;
  enableSearch?: boolean;
}

class OpenAIClient {
  traceInput = false;
  traceOutput = true;
  private _logger = Logger.create({ tag: "Open AI" });

  deployment?: string;

  private _client?: OpenAI;
  private _init() {
    this.deployment = kEnvs.AZURE_OPENAI_DEPLOYMENT;
    if (!this._client) {
      this._client = kEnvs.AZURE_OPENAI_API_KEY
        ? new AzureOpenAI({
            httpAgent: kProxyAgent,
            deployment: this.deployment,
          })
        : new OpenAI({ httpAgent: kProxyAgent });
    }
  }

  private _abortCallbacks: Record<string, VoidFunction> = {
    // requestId: abortStreamCallback
  };

  cancel(requestId: string) {
    this._init();
    if (this._abortCallbacks[requestId]) {
      this._abortCallbacks[requestId]();
      delete this._abortCallbacks[requestId];
    }
  }

  async chat(options: ChatOptions) {
    this._init();
    let {
      user,
      system,
      tools,
      jsonMode,
      requestId,
      trace = false,
      model = this.deployment ?? kEnvs.OPENAI_MODEL ?? "gpt-4o",
    } = options;
    if (trace && this.traceInput) {
      this._logger.log(
        `ğŸ”¥ onAskAI\nğŸ¤–ï¸ System: ${system ?? "None"}\nğŸ˜Š User: ${user}`.trim()
      );
    }
    const systemMsg: ChatCompletionMessageParam[] = isNotEmpty(system)
      ? [{ role: "system", content: system! }]
      : [];
    let signal: AbortSignal | undefined;
    if (requestId) {
      const controller = new AbortController();
      this._abortCallbacks[requestId] = () => controller.abort();
      signal = controller.signal;
    }
    const chatCompletion = await this._client!.chat.completions.create(
      {
        model,
        tools,
        messages: [...systemMsg, { role: "user", content: user }],
        response_format: jsonMode ? { type: "json_object" } : undefined,
      },
      { signal }
    ).catch((e) => {
      this._logger.error("LLM å“åº”å¼‚å¸¸", e);
      return null;
    });
    if (requestId) {
      delete this._abortCallbacks[requestId];
    }
    const message = chatCompletion?.choices?.[0]?.message;
    if (trace && this.traceOutput) {
      this._logger.log(`âœ… Answer: ${message?.content ?? "None"}`.trim());
    }
    return message;
  }

  async chatStream(
    options: ChatOptions & {
      onStream?: (text: string) => void;
    }
  ) {
    this._init();
    let {
      user,
      system,
      tools,
      jsonMode,
      requestId,
      onStream,
      trace = false,
      model = this.deployment ?? kEnvs.OPENAI_MODEL ?? "gpt-4o",
      enableSearch = kEnvs.QWEN_ENABLE_SEARCH,
    } = options;
    
    this._logger.log(`ğŸ”¥ å¼€å§‹æµå¼AIè¯·æ±‚ - è¯·æ±‚ID: ${requestId || "æ— "}, æ¨¡å‹: ${model}`);
    
    if (trace && this.traceInput) {
      this._logger.log(
        `ğŸ”¥ onAskAI\nğŸ¤–ï¸ System: ${system ?? "None"}\nğŸ˜Š User: ${user}`.trim()
      );
    }
    const systemMsg: ChatCompletionMessageParam[] = isNotEmpty(system)
      ? [{ role: "system", content: system! }]
      : [];
    
    this._logger.log(`ğŸ”¥ å‘é€è¯·æ±‚åˆ°OpenAI - å·¥å…·æ•°é‡: ${tools?.length || 0}, JSONæ¨¡å¼: ${jsonMode ? "æ˜¯" : "å¦"}, æœç´¢åŠŸèƒ½: ${enableSearch ? "æ˜¯" : "å¦"}`);
    
    const stream = await this._client!.chat.completions.create({
      model,
      tools,
      stream: true,
      messages: [...systemMsg, { role: "user", content: user }],
      response_format: jsonMode ? { type: "json_object" } : undefined,
      ...(enableSearch && { enable_search: true })
    }).catch((e) => {
      this._logger.error("LLM å“åº”å¼‚å¸¸", e);
      return null;
    });
    if (!stream) {
      this._logger.error(`ğŸ”¥ æµå¼è¯·æ±‚å¤±è´¥ - è¯·æ±‚ID: ${requestId || "æ— "}`);
      return;
    }
    if (requestId) {
      this._abortCallbacks[requestId] = () => stream.controller.abort();
    }
    
    this._logger.log(`ğŸ”¥ å¼€å§‹æ¥æ”¶æµå¼å“åº” - è¯·æ±‚ID: ${requestId || "æ— "}`);
    
    let content = "";
    let chunkCount = 0;
    for await (const chunk of stream) {
      const text = chunk.choices[0]?.delta?.content || "";
      const aborted =
        requestId && !Object.keys(this._abortCallbacks).includes(requestId);
      if (aborted) {
        this._logger.log(`ğŸ”¥ æµå¼å“åº”è¢«ä¸­æ­¢ - è¯·æ±‚ID: ${requestId}`);
        content = "";
        break;
      }
      if (text) {
        onStream?.(text);
        content += text;
        chunkCount++;
      }
    }
    if (requestId) {
      delete this._abortCallbacks[requestId];
    }
    
    this._logger.log(`ğŸ”¥ æµå¼å“åº”å®Œæˆ - è¯·æ±‚ID: ${requestId || "æ— "}, æ€»å—æ•°: ${chunkCount}, æ€»å­—ç¬¦æ•°: ${content.length}`);
    
    if (trace && this.traceOutput) {
      this._logger.log(`âœ… Answer: ${content ?? "None"}`.trim());
    }
    return withDefault(content, undefined);
  }
}

export const openai = new OpenAIClient();
